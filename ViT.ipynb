{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9793518,"sourceType":"datasetVersion","datasetId":5908465}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gensim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T18:24:41.779869Z","iopub.execute_input":"2024-11-04T18:24:41.780714Z","iopub.status.idle":"2024-11-04T18:25:05.071446Z","shell.execute_reply.started":"2024-11-04T18:24:41.780674Z","shell.execute_reply":"2024-11-04T18:25:05.070338Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.3)\nRequirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.26.4)\nCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n  Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (7.0.4)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\nDownloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.14.1\n    Uninstalling scipy-1.14.1:\n      Successfully uninstalled scipy-1.14.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scipy-1.13.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import scipy\nimport gensim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T18:25:05.073457Z","iopub.execute_input":"2024-11-04T18:25:05.073766Z","iopub.status.idle":"2024-11-04T18:25:05.078395Z","shell.execute_reply.started":"2024-11-04T18:25:05.073733Z","shell.execute_reply":"2024-11-04T18:25:05.077513Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Install required packages if not already installed\n!pip install transformers ","metadata":{"id":"uf5ijSo2ETXU","execution":{"iopub.status.busy":"2024-11-04T18:25:05.079506Z","iopub.execute_input":"2024-11-04T18:25:05.079789Z","iopub.status.idle":"2024-11-04T18:25:16.619323Z","shell.execute_reply.started":"2024-11-04T18:25:05.079759Z","shell.execute_reply":"2024-11-04T18:25:16.618153Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"category = \"Men_Tshirts\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T18:25:16.622548Z","iopub.execute_input":"2024-11-04T18:25:16.623273Z","iopub.status.idle":"2024-11-04T18:25:16.627549Z","shell.execute_reply.started":"2024-11-04T18:25:16.623225Z","shell.execute_reply":"2024-11-04T18:25:16.626799Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import ViTModel, ViTFeatureExtractor\nfrom torchvision import transforms\nfrom PIL import Image\nimport gensim\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\n\n# 1. Data Preparation\n# Load your dataset\n# Assume 'data.csv' has columns: 'image_path', 'len', 'color', 'neck', 'pattern', 'print_or_pattern_type', 'sleeve_length'\ndata = pd.read_csv(f'/kaggle/input/m1dataset/{category}.csv')\n# data.drop(\"attr_10\", axis=1, inplace=True)\n\n# Ensure the 'image_path' column contains the correct paths to your images\n# For example:\n# data['image_path'] = data['image_path'].apply(lambda x: os.path.join('images', x))","metadata":{"id":"Uw7OoCFlDumw","trusted":true,"execution":{"iopub.status.busy":"2024-11-04T18:25:16.629115Z","iopub.execute_input":"2024-11-04T18:25:16.629487Z","iopub.status.idle":"2024-11-04T18:25:16.683449Z","shell.execute_reply.started":"2024-11-04T18:25:16.629446Z","shell.execute_reply":"2024-11-04T18:25:16.682762Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T18:25:16.684470Z","iopub.execute_input":"2024-11-04T18:25:16.684781Z","iopub.status.idle":"2024-11-04T18:25:16.689081Z","shell.execute_reply.started":"2024-11-04T18:25:16.684749Z","shell.execute_reply":"2024-11-04T18:25:16.688121Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Word2Vec Embeddings\nfrom gensim.models import KeyedVectors\n\n# Load pre-trained word2vec embeddings (e.g., Google News vectors)\n# Download link: https://code.google.com/archive/p/word2vec/\n# Make sure you have 'GoogleNews-vectors-negative300.bin.gz' in your working directory\nword2vec_model = KeyedVectors.load_word2vec_format('/kaggle/input/m1dataset/GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin', binary=True)\n\n# Function to get embeddings for attribute values\ndef get_embedding(word):\n    word = str(word).lower().replace(' ', '_')\n    if word in word2vec_model:\n        return word2vec_model[word]\n    else:\n        # Handle out-of-vocabulary words\n        # You can use a random vector or zeros\n        return np.zeros(word2vec_model.vector_size)","metadata":{"id":"P3AAo0uNEPZr","trusted":true,"execution":{"iopub.status.busy":"2024-11-04T18:25:16.690291Z","iopub.execute_input":"2024-11-04T18:25:16.690696Z","iopub.status.idle":"2024-11-04T18:26:29.138561Z","shell.execute_reply.started":"2024-11-04T18:25:16.690655Z","shell.execute_reply":"2024-11-04T18:26:29.137566Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"data.columns[3:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T18:26:29.139856Z","iopub.execute_input":"2024-11-04T18:26:29.140163Z","iopub.status.idle":"2024-11-04T18:26:29.148064Z","shell.execute_reply.started":"2024-11-04T18:26:29.140131Z","shell.execute_reply":"2024-11-04T18:26:29.147172Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Index(['color', 'neck', 'pattern', 'print_or_pattern_type', 'sleeve_length',\n       'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10'],\n      dtype='object')"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# 3. Dataset Creation\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, img_dir, transform=None):\n        self.data = dataframe.reset_index(drop=True)\n        self.transform = transform\n        self.img_dir = img_dir\n        self.dataframe = dataframe\n        self.feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n        self.attributes = data.columns[3:8]\n        self.embedding_dim = word2vec_model.vector_size\n        self.num_attributes = len(self.attributes)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # Get image\n        img_path = os.path.join(self.img_dir, f\"{int(self.dataframe.iloc[idx, 0]):06d}.jpg\")\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        else:\n            # Use ViT's feature extractor transformations\n            image = self.feature_extractor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n\n        # Get attribute embeddings\n        embeddings = []\n        for attr in self.attributes:\n            value = self.data.loc[idx, attr]\n            embedding = get_embedding(value)\n            embeddings.append(embedding)\n        target = np.concatenate(embeddings)\n        target = torch.tensor(target, dtype=torch.float32)\n        return image, target\n\n# Define transformations (if needed)\n# ViT models expect images of size 224x224\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Split data into training and validation sets\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(data, test_size=0.1, random_state=42)\n# train_df = data\n\n# Create datasets and dataloaders\ntrain_dataset = CustomDataset(train_df, \"/kaggle/input/m1dataset/train_images\", transform=transform)\nval_dataset = CustomDataset(val_df, \"/kaggle/input/m1dataset/train_images\", transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)","metadata":{"id":"XBPI4f5PEMdP","trusted":true,"execution":{"iopub.status.busy":"2024-11-04T18:28:21.793676Z","iopub.execute_input":"2024-11-04T18:28:21.794523Z","iopub.status.idle":"2024-11-04T18:28:21.944968Z","shell.execute_reply.started":"2024-11-04T18:28:21.794482Z","shell.execute_reply":"2024-11-04T18:28:21.944093Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# 4. Model Definition\nclass ViTForAttributePrediction(nn.Module):\n    def __init__(self, embedding_dim, num_attributes):\n        super(ViTForAttributePrediction, self).__init__()\n        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n        self.fc = nn.Linear(self.vit.config.hidden_size, embedding_dim * num_attributes)\n        self.num_attributes = num_attributes\n        self.embedding_dim = embedding_dim\n\n    def forward(self, x):\n        outputs = self.vit(pixel_values=x)\n        pooled_output = outputs.pooler_output  # [batch_size, hidden_size]\n        logits = self.fc(pooled_output)        # [batch_size, embedding_dim * num_attributes]\n        logits = logits.view(-1, self.num_attributes, self.embedding_dim)\n        return logits\n\n# Instantiate the model\nembedding_dim = word2vec_model.vector_size\nnum_attributes = len(train_dataset.attributes)\nmodel = ViTForAttributePrediction(embedding_dim, num_attributes)\n\n# Move model to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n","metadata":{"id":"tdff3rZrEKVD","trusted":true,"execution":{"iopub.status.busy":"2024-11-04T18:28:25.854593Z","iopub.execute_input":"2024-11-04T18:28:25.855246Z","iopub.status.idle":"2024-11-04T18:28:26.212112Z","shell.execute_reply.started":"2024-11-04T18:28:25.855200Z","shell.execute_reply":"2024-11-04T18:28:26.211121Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"ViTForAttributePrediction(\n  (vit): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTSdpaAttention(\n            (attention): ViTSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (fc): Linear(in_features=768, out_features=1500, bias=True)\n)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# 5. Loss Function and Optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-5)","metadata":{"id":"f9ZfqWuDEGhV","trusted":true,"execution":{"iopub.status.busy":"2024-11-04T18:28:27.421165Z","iopub.execute_input":"2024-11-04T18:28:27.421899Z","iopub.status.idle":"2024-11-04T18:28:27.428913Z","shell.execute_reply.started":"2024-11-04T18:28:27.421861Z","shell.execute_reply":"2024-11-04T18:28:27.428032Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Early Stopping Parameters\nearly_stopping_patience = 3  # Number of epochs to wait before stopping\nbest_val_loss = float('inf')\nepochs_no_improve = 0\n\n# 6. Training Loop\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for images, targets in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n        images = images.to(device)\n        targets = targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)  # outputs: [batch_size, num_attributes, embedding_dim]\n        targets = targets.view(-1, num_attributes, embedding_dim)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n    epoch_loss = running_loss / len(train_loader.dataset)\n    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}')\n\n    # Validation Loop\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for images, targets in tqdm(val_loader, desc=\"Validation\"):\n            images = images.to(device)\n            targets = targets.to(device)\n            outputs = model(images)\n            targets = targets.view(-1, num_attributes, embedding_dim)\n            loss = criterion(outputs, targets)\n            val_loss += loss.item() * images.size(0)\n    val_loss /= len(val_loader.dataset)\n    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')\n\n    # Early Stopping Check\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        epochs_no_improve = 0\n        # Save the best model\n        torch.save(model.state_dict(), f'/kaggle/working/{category}_best_model.pth')\n        print(f'Validation loss improved. Model saved.')\n    else:\n        epochs_no_improve += 1\n        print(f'No improvement in validation loss for {epochs_no_improve} epoch(s).')\n\n    if epochs_no_improve >= early_stopping_patience:\n        print(f'Early stopping triggered. No improvement in validation loss for {early_stopping_patience} consecutive epochs.')\n        break","metadata":{"id":"uK2RXIUdEEP_","trusted":true,"execution":{"iopub.status.busy":"2024-11-04T18:28:29.122330Z","iopub.execute_input":"2024-11-04T18:28:29.122742Z"}},"outputs":[{"name":"stderr","text":"Training Epoch 1/15: 100%|██████████| 409/409 [04:46<00:00,  1.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15, Training Loss: 0.0140\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 46/46 [00:17<00:00,  2.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15, Validation Loss: 0.0109\nValidation loss improved. Model saved.\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2/15: 100%|██████████| 409/409 [04:13<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/15, Training Loss: 0.0106\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 46/46 [00:11<00:00,  3.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/15, Validation Loss: 0.0103\nValidation loss improved. Model saved.\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3/15:  41%|████      | 167/409 [01:43<02:29,  1.62it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!rm -rf *1.pth\n!rm -rf *2.pth\n!rm -rf *3.pth\n!rm -rf *4.pth\n!rm -rf *5.pth\n!rm -rf *6.pth\n!rm -rf *7.pth\n!rm -rf *8.pth\n!rm -rf *9.pth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T18:18:51.659483Z","iopub.execute_input":"2024-11-04T18:18:51.660343Z","iopub.status.idle":"2024-11-04T18:19:04.349178Z","shell.execute_reply.started":"2024-11-04T18:18:51.660288Z","shell.execute_reply":"2024-11-04T18:19:04.347332Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# 7. Evaluation\n# For evaluation, you can compute the cosine similarity between predicted embeddings and actual embeddings\ndef cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n# Example on the validation set\n# model.eval()\n# with torch.no_grad():\n#     for images, targets in val_loader:\n#         images = images.to(device)\n#         outputs = model(images)\n#         outputs = outputs.cpu().numpy()\n#         targets = targets.view(-1, num_attributes, embedding_dim).cpu().numpy()\n#         for i in range(outputs.shape[0]):\n#             for j in range(num_attributes):\n#                 pred_embedding = outputs[i, j, :]\n#                 true_embedding = targets[i, j, :]\n#                 similarity = cosine_similarity(pred_embedding, true_embedding)\n#                 print(f'Attribute {train_dataset.attributes[j]} Similarity: {similarity:.4f}')","metadata":{"id":"Cws2aFdlEAq9","trusted":true,"execution":{"iopub.status.busy":"2024-11-03T08:53:05.599938Z","iopub.execute_input":"2024-11-03T08:53:05.600239Z","iopub.status.idle":"2024-11-03T08:53:05.606697Z","shell.execute_reply.started":"2024-11-03T08:53:05.600205Z","shell.execute_reply":"2024-11-03T08:53:05.605638Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# Save the trained model\n# torch.save(model.state_dict(), '/kaggle/working/2_best_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T08:53:05.607881Z","iopub.execute_input":"2024-11-03T08:53:05.608192Z","iopub.status.idle":"2024-11-03T08:53:05.618441Z","shell.execute_reply.started":"2024-11-03T08:53:05.608159Z","shell.execute_reply":"2024-11-03T08:53:05.617602Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Initialize the model architecture\nmodel = ViTForAttributePrediction(\n    embedding_dim=embedding_dim,\n    num_attributes=num_attributes\n)\nmodel.to(device)\n\n# Load the saved model weights\nmodel.load_state_dict(torch.load(f'/kaggle/working/{category}_best_model.pth'))\nmodel.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test data\ntest_data = pd.read_csv(f'/kaggle/input/m1dataset/test_folder/test_folder/test_{category}.csv')\n\n# # Create a column for image paths based on 'id' and 'Category'\n# # Adjust the path construction according to your directory structure\n# test_data['image_path'] = test_data.apply(\n#     lambda row: os.path.join('images', row['Category'], f\"{row['id']}.jpg\"),\n#     axis=1\n# )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, dataframe, img_dir, transform=None):\n        self.img_dir = img_dir\n        self.dataframe = dataframe\n        self.data = dataframe.reset_index(drop=True)\n        self.transform = transform\n        self.feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        # Get image\n        img_path = os.path.join(self.img_dir, f\"{int(self.dataframe.iloc[idx, 0]):06d}.jpg\")\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        else:\n            image = self.feature_extractor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n        return image\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = TestDataset(test_data, \"/kaggle/input/m1dataset/test_images\", transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of attributes to predict\nattributes = ['color', 'neck', 'pattern', 'print_or_pattern_type', 'sleeve_length']\n\n# Create dictionaries for each attribute\nattribute_value_embeddings = {}\n\nfor attr in attributes:\n    # Get unique attribute values from training data\n    unique_values = data[attr].dropna().unique()\n    embeddings = []\n    for value in unique_values:\n        embedding = get_embedding(value)\n        embeddings.append(embedding)\n    attribute_value_embeddings[attr] = {\n        'values': unique_values,\n        'embeddings': np.stack(embeddings)\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_closest_attribute(embedding, attr):\n    embeddings = attribute_value_embeddings[attr]['embeddings']  # [num_values, embedding_dim]\n    values = attribute_value_embeddings[attr]['values']\n    # Compute cosine similarity\n    similarities = np.dot(embeddings, embedding) / (\n        np.linalg.norm(embeddings, axis=1) * np.linalg.norm(embedding) + 1e-10\n    )\n    # Get the index of the most similar embedding\n    idx = np.argmax(similarities)\n    return values[idx]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize lists to store predictions\npredictions = []\n\nmodel.eval()\nwith torch.no_grad():\n    for batch_images in tqdm(test_loader, desc=\"Testing\"):\n        batch_images = batch_images.to(device)\n        outputs = model(batch_images)  # [batch_size, num_attributes, embedding_dim]\n        outputs = outputs.cpu().numpy()\n        \n        batch_size = outputs.shape[0]\n        for i in range(batch_size):\n            sample_predictions = {}\n\n            for j, attr in enumerate(attributes):\n                # print(attr)\n                pred_embedding = outputs[i, j, :]  # Predicted embedding for attribute\n                # Find the closest attribute value\n                pred_value = find_closest_attribute(pred_embedding, attr)\n                sample_predictions[attr] = pred_value\n            predictions.append(sample_predictions)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert predictions to DataFrame\npredictions_df = pd.DataFrame(predictions)\n\n# Concatenate with test_data\ntest_results = pd.concat([test_data[['id', 'Category']].reset_index(drop=True), predictions_df], axis=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save to CSV\ntest_results.to_csv(f'test_predictions_new_model_{category}.csv', index=False)\n\nprint(\"Predictions saved to 'test_predictions.csv'\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf *9.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T04:30:33.392362Z","iopub.execute_input":"2024-11-03T04:30:33.393297Z","iopub.status.idle":"2024-11-03T04:30:34.593065Z","shell.execute_reply.started":"2024-11-03T04:30:33.393251Z","shell.execute_reply":"2024-11-03T04:30:34.591697Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}